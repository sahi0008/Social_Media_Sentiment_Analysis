{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbfa4c05-7d4b-4dd3-aa5c-d4cf29adaa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec9774c-4539-4116-8761-df16ae40bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c078d8-28a9-4338-a674-9bba230a910d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc28ff-e974-437c-9762-ae0c4649ad41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d751a-437b-44c4-832f-d6e5177bb988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98febf12-f435-4581-817d-61a6c4f4b350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38d0ea9f-5549-41e4-b4ba-57f6c568f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import urllib.request\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb3a5320-d788-4318-86e5-7d9e7a67e2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Fix random seeds\n",
    "torch.manual_seed(7)\n",
    "np.random.seed(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0188e418-4081-462f-b5f0-a9b566a95689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\souvik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK punkt tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52f08b5a-c8df-4738-aaa6-2fedebfaaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download IMDB dataset if not exists\n",
    "if not os.path.exists('aclImdb'):\n",
    "    print(\"Downloading IMDB dataset...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
    "        'aclImdb_v1.tar.gz'\n",
    "    )\n",
    "    with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "        tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2df97bb6-3734-47e8-8daa-f0577c877308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read IMDB dataset\n",
    "def load_imdb_data(data_path):\n",
    "    texts, labels = [], []\n",
    "    for label_type in ['pos', 'neg']:\n",
    "        dir_path = os.path.join(data_path, label_type)\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            with open(os.path.join(dir_path, file_name), encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(1 if label_type == 'pos' else 0)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0146385-7cb3-4757-b048-e7069d6f2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = load_imdb_data('aclImdb/train')\n",
    "test_texts, test_labels = load_imdb_data('aclImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1877fe7-a028-44db-842f-6a2b90428cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Tokenization and Cleaning\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "train_tokens = [preprocess(text) for text in train_texts]\n",
    "test_tokens = [preprocess(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aa9f85f-b6f3-448a-bd1d-225e7c19f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary\n",
    "all_tokens = train_tokens + test_tokens\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for tokens in all_tokens:\n",
    "    for token in tokens:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b29ff48d-1e8e-4615-a9ab-28bc3f6785e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to vocab.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the vocabulary to a file\n",
    "torch.save(vocab, 'vocab.pt')  # Save vocab.pt\n",
    "\n",
    "print(\"Vocabulary saved to vocab.pt\")\n",
    "# Load the saved vocabulary\n",
    "#vocab = torch.load('vocab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de54aef4-e379-42d9-a19a-91a2923797d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sequences\n",
    "def encode(tokens, vocab):\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aa39fca-4fc4-4b78-b030-a06db405caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [encode(tokens, vocab) for tokens in train_tokens]\n",
    "test_sequences = [encode(tokens, vocab) for tokens in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "817baad6-806c-4a19-9f9f-9d531a9df4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Dataset\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9b11441-a052-4016-abff-3147d5c85279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad batches\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return sequences_padded.to(device), labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d342ff5-cadd-407e-9546-fa6a9dde3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = IMDBDataset(train_sequences, train_labels)\n",
    "test_dataset = IMDBDataset(test_sequences, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9312210b-076a-42a5-9165-2ca5e7ab7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM Model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1796b49f-bd03-4112-9798-af1625d870ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved LSTM Model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_layers=1, bidirectional=True):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                          # (batch_size, seq_len, embed_dim)\n",
    "        lstm_out, (hidden, _) = self.lstm(x)           # hidden: (num_layers * num_directions, batch, hidden_dim)\n",
    "\n",
    "        # Concatenate final forward and backward hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # (batch, hidden_dim * 2)\n",
    "        else:\n",
    "            hidden = hidden[-1]                                  # (batch, hidden_dim)\n",
    "\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc(hidden)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b324a5a-aa3a-4a5c-b8ad-9959b31167d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2b16d8e-1fcf-44fa-a4df-9e9933a9e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentLSTM(vocab_size, embed_dim, hidden_dim, output_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38d939a0-7abf-4640-8a58-cde92650d8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  4 20:59:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 561.19                 Driver Version: 561.19         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   48C    P8              1W /   30W |    3913MiB /   4096MiB |     12%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     15644      C   C:\\Users\\souvik\\anaconda3\\python.exe        N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f27f297d-62bc-481b-92ae-0571ceeb4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b5f53-0429-4dff-a904-2bfb49d6d72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03eabde2-a945-4601-aab1-ec5544983141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 0.6429 - Accuracy: 61.94% - Time: 1m 33s\n",
      "Epoch [2/10] - Loss: 0.5675 - Accuracy: 71.80% - Time: 1m 30s\n",
      "Epoch [3/10] - Loss: 0.6076 - Accuracy: 65.59% - Time: 1m 30s\n",
      "Epoch [4/10] - Loss: 0.4904 - Accuracy: 76.49% - Time: 1m 31s\n",
      "Epoch [5/10] - Loss: 0.3280 - Accuracy: 87.20% - Time: 1m 30s\n",
      "Epoch [6/10] - Loss: 0.2120 - Accuracy: 92.47% - Time: 1m 29s\n",
      "Epoch [7/10] - Loss: 0.1436 - Accuracy: 95.33% - Time: 1m 32s\n",
      "Epoch [8/10] - Loss: 0.0983 - Accuracy: 97.06% - Time: 1m 31s\n",
      "Epoch [9/10] - Loss: 0.0673 - Accuracy: 98.16% - Time: 1m 29s\n",
      "Epoch [10/10] - Loss: 0.0467 - Accuracy: 98.81% - Time: 1m 31s\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start timer\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for sequences, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    epoch_time = end_time - start_time\n",
    "    minutes = int(epoch_time // 60)\n",
    "    seconds = int(epoch_time % 60)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(train_loader):.4f} - Accuracy: {100*correct/total:.2f}% - Time: {minutes}m {seconds}s\")\n",
    "\n",
    "print(\"Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1736bef8-1904-4809-a25a-725df04a8a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.81%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        outputs = model(sequences)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b361ba42-5326-4bcc-a9e2-7b67f7fe10c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'sentiment_lstm_new.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"sentiment_lstm_new.pth\")\n",
    "print(\"Model saved as 'sentiment_lstm_new.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa6f8911-c7e4-4228-8220-377cc7eaf563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function\n",
    "def predict(text):\n",
    "    input_tensor = preprocess_and_encode(text, vocab).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0][pred].item()\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    return sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b2674fc-6fd7-4e4e-8d5f-411838cb18bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Positive (88.15% confidence)\n"
     ]
    }
   ],
   "source": [
    "text = \"This movie was absolutely fantastic!\"\n",
    "sentiment, confidence = predict(text)\n",
    "print(f\"Predicted: {sentiment} ({confidence*100:.2f}% confidence)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b389445-622c-42a1-82f1-00fcfec34445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocessing and token encoding function\n",
    "def preprocess_and_encode(text, vocab, max_len=300):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    ids = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens[:max_len]]\n",
    "    tensor = torch.tensor(ids, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b956b0b7-f55c-4683-bc71-22193170c495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\souvik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\souvik\\AppData\\Local\\Temp\\ipykernel_15644\\3798133223.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vocab = torch.load(vocab_path)\n",
      "C:\\Users\\souvik\\AppData\\Local\\Temp\\ipykernel_15644\\3798133223.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# app.py\n",
    "import streamlit as st\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load vocab and model\n",
    "vocab_path = \"vocab.pt\"\n",
    "model_path = \"sentiment_lstm_new.pth\"\n",
    "\n",
    "if not os.path.exists(vocab_path) or not os.path.exists(model_path):\n",
    "    st.error(\"Model or vocabulary file not found.\")\n",
    "    st.stop()\n",
    "\n",
    "vocab = torch.load(vocab_path)\n",
    "\n",
    "# Model definition\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_layers=1, bidirectional=True):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# Load model (set output_dim = 2 for binary classification)\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Binary classification (Positive/Negative)\n",
    "\n",
    "model = SentimentLSTM(vocab_size, embed_dim, hidden_dim, output_dim).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_and_encode(text, vocab, max_len=300):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    ids = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens[:max_len]]\n",
    "    tensor = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "    return tensor\n",
    "\n",
    "# Prediction\n",
    "def predict(text):\n",
    "    input_tensor = preprocess_and_encode(text, vocab).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0][pred].item()\n",
    "    sentiment_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "    return sentiment_map[pred], confidence\n",
    "\n",
    "# Streamlit GUI\n",
    "st.title(\"Sentiment Analysis App\")\n",
    "st.write(\"Enter a sentence and get sentiment prediction:\")\n",
    "\n",
    "user_input = st.text_area(\"Input Text\")\n",
    "\n",
    "if st.button(\"Predict Sentiment\"):\n",
    "    if user_input.strip():\n",
    "        sentiment, confidence = predict(user_input)\n",
    "        st.markdown(f\"### Sentiment: {sentiment}\")\n",
    "        st.markdown(f\"**Confidence:** {confidence*100:.2f}%\")\n",
    "    else:\n",
    "        st.warning(\"Please enter some text to analyze.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c92f9a6c-5944-4268-8900-4d0f785f0d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\souvik\\AppData\\Local\\Temp\\ipykernel_15644\\1983491817.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vocab = torch.load(vocab_path)  # Assuming vocab is just a dictionary, no need for weights_only=True here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(166760, 128)\n",
       "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load vocab and model\n",
    "vocab_path = \"vocab.pt\"\n",
    "model_path = \"sentiment_lstm_new.pth\"\n",
    "\n",
    "# Ensure the files exist\n",
    "if not os.path.exists(vocab_path) or not os.path.exists(model_path):\n",
    "    st.error(\"Model or vocabulary file not found.\")\n",
    "    st.stop()\n",
    "\n",
    "# Load vocab and model weights only (avoid executing unsafe code)\n",
    "vocab = torch.load(vocab_path)  # Assuming vocab is just a dictionary, no need for weights_only=True here\n",
    "\n",
    "# Load model architecture and weights\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Binary classification (Positive/Negative)\n",
    "\n",
    "model = SentimentLSTM(vocab_size, embed_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Use weights_only=True to avoid loading unnecessary objects\n",
    "model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791ec3a-2039-4e08-8b2b-f4a8eb0be241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74b9f9-c9fe-474f-8433-844e75318f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
